{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "# Read the CSV file into a DataFrame\n",
    "file_path = r'C:\\Users\\LeAnhNguyen\\Desktop\\Obeta\\003 pick_data.csv'\n",
    "dtype_spec = {'product_id': int, 'warehouse_section': str, 'origin': int,␣\n",
    "↪'order_number': int, 'position_in_order': str, 'pick_volume': int,␣\n",
    "↪'quantity_unit': str, 'date': str}\n",
    "df = pd.read_csv(file_path, header = None , dtype=dtype_spec, low_memory=False)\n",
    "# Name the header\n",
    "df.columns = [\"product_id\", \"warehouse_section\", \"origin\", \"order_number\",␣\n",
    "↪\"position_in_order\",\"pick_volume\",\"quantity_unit\",\"date\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate\n",
    "df = df.drop_duplicates()\n",
    "#Check number of rows and columns after remove duplicate\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where pick volume = negative or pick volume = 0\n",
    "df = df[df['pick_volume']> 0]\n",
    "print(df)\n",
    "##Check number of rows and columns after remove pick volume <= 0\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to identify outliers by using 3 Standard Deviations method\n",
    "def find_group_outliers(group):\n",
    "mean = group.mean()\n",
    "std = group.std()\n",
    "threshold = 3 * std\n",
    "outliers = group[abs(group - mean) > threshold]\n",
    "return len(outliers), outliers.index.tolist() # Return the count and␣\n",
    "↪indices of outliers\n",
    "# Use groupby to apply the function to each product_id\n",
    "grouped_outliers_info = df.groupby('product_id')['pick_volume'].apply(find_group_outliers)\n",
    "# Create a list of row number(index of row) of outliers, It will be refered to when erasing outliers\n",
    "all_outliers_indices = [index for _, indices in grouped_outliers_info for index in indices]\n",
    "#count the total number of outliers\n",
    "total_outliers_count = sum(count for count, _ in grouped_outliers_info)\n",
    "# Erase the rows containing outliers\n",
    "df_cleaned = df.drop(index=all_outliers_indices)\n",
    "# Show the cleaned DataFrame and total number of outliers\n",
    "print(\"Cleaned DataFrame:\")\n",
    "print(df_cleaned)\n",
    "print(\"\\nTotal number of outliers:\", total_outliers_count)\n",
    "#Check number of rows and columns after remove outliers\n",
    "print('\\nShape of the cleaned df:',df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b23bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for univariate analysis of nominal columns\n",
    "def univariate_nominal(group):\n",
    "count = group.count()\n",
    "unique_count = group.nunique()\n",
    "mode_value = group.mode().iloc[0]\n",
    "mode_frequency = group.value_counts().iloc[0] # Add this line to calculate␣\n",
    "↪mode frequency\n",
    "return count, unique_count, mode_value, mode_frequency\n",
    "# List of columns to analyze\n",
    "columns_to_analyze = ['product_id', 'warehouse_section', 'origin','order_number', 'quantity_unit']\n",
    "# Dictionary to store results\n",
    "results_nominal = {}\n",
    "# Applying the function to each column\n",
    "for column in columns_to_analyze:\n",
    "results_nominal[column] = univariate_nominal(df_cleaned[column])\n",
    "# Displaying the results\n",
    "for column, result in results_nominal.items():\n",
    "print(f\"Univariate Analysis for {column}:\")\n",
    "print(f\"Count: {result[0]}\")\n",
    "print(f\"Unique Count: {result[1]}\")\n",
    "print(f\"Mode Value: {result[2]}\")\n",
    "print(f\"Mode Frequency: {result[3]}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for univariate analysis of pick_vol column\n",
    "def descriptive_stats_numeric(group):\n",
    "stats = {\n",
    "'Count': group.count(),\n",
    "'Unique Count': group.nunique(),\n",
    "'Maximum': group.max(),\n",
    "'Minimum': group.min(),\n",
    "'Median': group.median(),\n",
    "'Mode': group.mode().iloc[0] if not group.mode().empty else None,\n",
    "'Mode Frequency': group.value_counts().max() if not group.mode().empty\n",
    "    else None,\n",
    "'Mean': group.mean(), # Added line for mean calculation\n",
    "'25%': group.quantile(0.25),\n",
    "'50%': group.quantile(0.5),\n",
    "'75%': group.quantile(0.75),\n",
    "'IQR': group.quantile(0.75) - group.quantile(0.25),\n",
    "'Std': group.std(),\n",
    "'Variance': group.var(),\n",
    "}\n",
    "return stats\n",
    "# Apply the function to the pick_volume column\n",
    "numeric_stats_result = descriptive_stats_numeric(df_cleaned['pick_volume'])\n",
    "# Show the result\n",
    "print(\"Descriptive Statistics for Numeric Variable:\")\n",
    "print(numeric_stats_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    ": #Create a new order number column with prefix because of repeating order number issue\n",
    "# Convert 'order_date' to datetime format\n",
    "df_cleaned['date'] = pd.to_datetime(df_cleaned['date'])\n",
    "# Extract the year from 'order_date'\n",
    "df_cleaned['year'] = df_cleaned['date'].dt.year\n",
    "# Create the 'adjusted order number' column with the prefix of the year\n",
    "df_cleaned['adjusted_order_number'] = df_cleaned['year'].astype(str) + '_' + df_cleaned['order_number'].astype(str)\n",
    "# Displaying the DataFrame with the new column\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0acdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 2nd CSV file into a DataFrame\n",
    "file_path = r'C:\\Users\\LeAnhNguyen\\Desktop\\Obeta\\002 product_data UTF8.txt'\n",
    "df1 = pd.read_csv(file_path)\n",
    "df1.columns = [\"product_id\", \"description\", \"product_group\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3db40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace German umlauts in the 'City' column\n",
    "df1['product_group'] = df1['product_group'].str.replace('ü', 'ue').str.replace('ö', 'oe').str.replace('ä', 'ae')\n",
    "df1['description'] = df1['description'].str.replace('ü', 'ue').str.replace('ö','oe').str.replace('ä', 'ae')\n",
    "# Display the result\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf316ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ER model\n",
    "# There are 1 Fact Table named 'Aggregated Fact Table', and 2 Dimensional Tables named, Product Table, and Order Table.\n",
    "# Steps to create the Product Table\n",
    "# Filter by columns 'product_id', 'quantity_unit'\n",
    "columns_to_keep = ['product_id', 'quantity_unit']\n",
    "# Select specific columns from df_cleaned\n",
    "df_cleaned_selected = df_cleaned[columns_to_keep].copy()\n",
    "# Remove duplicates based on the selected columns\n",
    "df_cleaned_selected = df_cleaned_selected.drop_duplicates()\n",
    "# Merge with df1 using left join, df1 is the 2nd imported .csv file which is␣\n",
    "↪product data\n",
    "product_1st_draft = df1.merge(df_cleaned_selected, how='left', on='product_id')\n",
    "# Show 1st draft of Product Table\n",
    "product_1st_draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Steps to create the Order Table\n",
    "# Filter by columns 'adjusted_order_number','order_number','origin','position_in_order'\n",
    "columns_to_keep1 = ['adjusted_order_number','order_number','origin','position_in_order']\n",
    "# Select specific columns from df_cleaned\n",
    "order_no_dup = df_cleaned[columns_to_keep1].copy()\n",
    "# Remove duplicates based on the selected columns\n",
    "order_no_dup = order_no_dup.drop_duplicates()\n",
    "# Show 1st draft of Order Table\n",
    "order_no_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc33c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "order= order_no_dup\n",
    "#Concatenate 3 columns from df1 and 1 column from product_1st_draft\n",
    "product= pd.concat([df1['product_id'], product_1st_draft['quantity_unit'],df1['product_group'],df1['description']], axis=1)\n",
    "# Displaying the separated tables\n",
    "print(\"\\norder:\")\n",
    "print(order)\n",
    "print(\"\\nproduct:\")\n",
    "print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    ": # Create the Aggregated Fact Table\n",
    "# Create 2 new columns to help minimizing Tableau computing time\n",
    "aggregated_pick = df_cleaned.groupby(['adjusted_order_number', 'product_id','date', 'warehouse_section']).agg(\n",
    "sum_pick_volume=pd.NamedAgg(column='pick_volume', aggfunc='sum'),\n",
    "count_order_number=pd.NamedAgg(column='adjusted_order_number',aggfunc='count')\n",
    ").reset_index()\n",
    "aggregated_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime type\n",
    "aggregated_pick['date'] = pd.to_datetime(aggregated_pick['date'])\n",
    "# Sort the DataFrame by 'order_number' and 'date'\n",
    "aggregated_pick = aggregated_pick.sort_values(by=['adjusted_order_number','date','warehouse_section'])\n",
    "# Calculate the lead time between consecutive picks for each 'order_number' and 'product_id'\n",
    "aggregated_pick['lead_time'] = aggregated_pick.groupby(['adjusted_order_number', 'warehouse_section'])['date'].diff().dt.total_seconds()\n",
    "# Calculate the average lead time per unit pick volume for each 'product_id'\n",
    "aggregated_pick['avg_lead_time_per_pick'] = aggregated_pick['lead_time'] / aggregated_pick['sum_pick_volume']\n",
    "# Display the DataFrame with lead times\n",
    "aggregated_pick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d23813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'product' with 'avg_lead_time_per_pick_by_product' based on 'product_id'\n",
    "product = product.merge(\n",
    "aggregated_pick.groupby('product_id')['avg_lead_time_per_pick'].mean().reset_index(),\n",
    "how='left',\n",
    "on='product_id',\n",
    "suffixes=('', '_new') # Use a suffix to distinguish the columns\n",
    ")\n",
    "# Drop unnecessary column\n",
    "product = product.drop(columns=['avg_lead_time_per_pick'])\n",
    "# Display the result\n",
    "product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by these columns\n",
    "pick_columns = ['adjusted_order_number','product_id','date','warehouse_section',\n",
    "'sum_pick_volume','count_order_number','lead_time',]\n",
    "agg_pick= aggregated_pick[pick_columns]\n",
    "# Display Aggregated Fact Table\n",
    "agg_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f523101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for connecting to MySQL\n",
    "!pip install mysql-connector-python\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = 'localhost'\n",
    "db_name = 'obeta_production'\n",
    "u_name = 'root'\n",
    "u_pass = 'mysql'\n",
    "port_num = 3306\n",
    "# Convert port_num to a string\n",
    "port_num_str = str(port_num)\n",
    "#create_engine('mysql+mysqlconnector://' + [root]:[mysql]@[localhost]:[3306]/[obeta_production]',echo=False)\n",
    "my_eng = create_engine('mysql+mysqlconnector://' + u_name + ':'+ u_pass + '@' + host_name + ':' + port_num_str + '/' + db_name ,echo=False)\n",
    "Session = sessionmaker(bind=my_eng)\n",
    "session = Session()\n",
    "try:\n",
    "# Perform database operations within the transaction\n",
    "product.to_sql(name='product', con=my_eng, if_exists = 'append', index = False, chunksize=1000)\n",
    "agg_pick.to_sql(name='agg_pick', con=my_eng, if_exists = 'append', index = False, chunksize=1000)\n",
    "order.to_sql(name='orders', con=my_eng, if_exists = 'append', index = False, chunksize=1000)\n",
    "# always commit changes!\n",
    "session.commit()\n",
    "except: # Handle any exceptions that occur during the transaction\n",
    "# Explicitly rollback the transaction\n",
    "if session:\n",
    "session.rollback()\n",
    "raise\n",
    "finally:\n",
    "# Close the connection\n",
    "if session:\n",
    "session.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
